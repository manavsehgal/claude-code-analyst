# Active Backlog

[x] Scaffold a minimal Python project structure with scripts/ folder where Python scripts will reside, tests/ folder for tests, docs/ folder for user guides. The project is managed by [uv](https://docs.astral.sh/uv/) for Python virtual environment and dependencies management
    **Completed**: Initialized uv project with `pyproject.toml` configured for claude-code-analyst (Python >=3.13). Created required directory structure: scripts/, tests/, docs/. Project now has proper Python project scaffolding with uv managing virtual environment and dependencies.

[x] Read [Claude Code best practices](https://www.anthropic.com/engineering/claude-code-best-practices) and write a minimal CLAUDE.md file for the project
    **Completed**: Reviewed Claude Code best practices from the official documentation. Created CLAUDE.md file with essential configuration including: common commands (development setup, linting, type checking), project structure overview, code style guidelines following PEP 8, testing instructions using pytest, repository etiquette, and developer environment setup requirements (Python >=3.13, uv package management).

[x] Create an article_to_md Python script to perform following actions: 
1. take a HTML web page url as input and scrapes the main article
2. check this is a valid HTML and respect robots.txt when scraping
3. ignore everything else including navigation, sidebar, footer, ads
4. create a destination folder markdown/kebab-case-title/
5. convert the scraped article as valid markdown which is well-formatted like original source 
6. save the markdown in the destination folder
7. download images within the article, save in images/ folder wihin destination folder
8. embed the relative references to the images within the markdown in same place as in the original source
    **Completed**: Created `scripts/article_to_md.py` script with full functionality for converting web articles to markdown. Implemented using readability-lxml for article extraction, beautifulsoup4 for HTML parsing, markdownify for markdown conversion, and requests for web scraping. The script respects robots.txt, validates HTML, extracts main article content, creates kebab-case directories, downloads images to an images/ subfolder, and properly embeds relative image references in the markdown. Added necessary dependencies to pyproject.toml: requests, beautifulsoup4, markdownify, and readability-lxml. Script tested successfully with example.com.

[x] Update article_to_md script to add metadata to markdown including article date if available, date scraped, article length in words, number of images, source url, article title. Article should have a level 1 heading matching the title.
    **Completed**: Enhanced scripts/article_to_md.py to include comprehensive metadata in YAML frontmatter format. Added extract_article_date() function that checks multiple meta tags (article:published_time, datePublished, etc.) and time tags for publication dates. Implemented word counting functionality with count_words(). Modified process_images() to return image count. Updated main function to collect all metadata (title, source URL, article date if available, scraping date, word count, image count) and format it as YAML frontmatter at the top of the markdown file. Added the article title as a level 1 heading after the metadata. Enhanced console output to display word count and image count statistics. Tested successfully with example.com.

[x] Make the article_to_md script available as a tool use for Claude Code by updating CLAUDE.md crisply and referencing docs/article-to-md-guide.md if required.
    **Completed**: Added "Available Tools" section to CLAUDE.md with comprehensive documentation for the article_to_md script. Updated docs/article-to-md-guide.md to include new metadata features (YAML frontmatter with title, source URL, article date, scrape date, word count, and image count). The tool is now properly documented and available for Claude Code usage with clear usage examples and reference to the detailed guide.

[x] Update the scripts/article_to_md.py to not add extra newlines to wrap text in markdown and only follow the HTML formatting to decide when to add newlines.
    **Completed**: Removed artificial text wrapping from the markdownify conversion by eliminating `wrap=True` and `wrap_width=80` parameters from the `md()` function call in `convert_to_markdown()`. The script now preserves natural text flow based on HTML structure rather than forcing line breaks at fixed character widths. Verified with example.com that text now flows naturally without arbitrary wrapping, maintaining readability while respecting the original HTML formatting for newline placement.

[x] Study the custom slash command .claude/commands/mermaid.md to add its definition to CLAUDE.md as a custom tool that is available to Claude Code
    **Completed**: Reviewed the mermaid custom command definition in .claude/commands/mermaid.md. Added comprehensive documentation to CLAUDE.md under "Available Tools" section, describing it as "Mermaid Visualization Generator (Custom Command)". Documented its features including: analyzing markdown content, creating multiple Mermaid.js diagrams (flowcharts, timelines, mindmaps), organizing output in mermaid/kebab-case-title/ folders, and including contextual text with visualizations. Provided usage syntax (/mermaid <markdown-file-path>) and detailed the 5-step automatic process the command follows.

[x] Update scripts/article_to_md.py to also work with downloaded html path as source.
    **Completed**: Enhanced article_to_md.py script to accept both URLs and local HTML file paths as input. Added is_url() function to detect input type and read_local_html() function to read local files. Modified main() function to handle both cases - URLs are fetched via HTTP while local files are read directly from disk. The script automatically skips robots.txt check for local files and properly handles relative/absolute file paths. Updated argument parser to accept "source" parameter instead of "url" with updated help text. Tested successfully with both URL inputs (maintains existing functionality) and local HTML files (new functionality). The script now works seamlessly with both web articles and downloaded HTML files.

[x] Update CLAUDE.md and docs/article-to-md-guide.md with revised capabilities of scripts/article_to_md.py to handle local HTML file paths.
    **Completed**: Updated both CLAUDE.md and docs/article-to-md-guide.md to fully document the enhanced capabilities of article_to_md.py. In CLAUDE.md, modified the "Article to Markdown Converter" section to show usage with both web URLs and local HTML files, updated command examples, and enhanced the features list to highlight dual input support and automatic detection. In docs/article-to-md-guide.md, made comprehensive updates including: updated overview and features sections, added separate usage examples for web URLs and local files, changed command-line parameter from "url" to "source", enhanced metadata format documentation, restructured "How It Works" section with input detection and dual content retrieval, added new "Working with Local HTML Files" section covering use cases, file path handling, image processing, and integration with html_downloader.py, updated examples with local file scenarios, enhanced error handling with file-specific errors, added comprehensive batch processing examples for mixed sources, and updated limitations section to distinguish between web and local file constraints. Both documentation files now provide complete guidance for using the enhanced script with either web articles or local HTML files.

[x] Create a script scripts/mermaid_to_image.py which takes a markdown with mermaid script as input and outputs a high fidelity image based on the mermaid visualization. Create a `config.yml` config file with user configurable defaults like height, width, dpi, destination folder for the images. Use visualizations/ folder as default. Traverse parent folder of the source mermaid markdown file and create folder with same name under image destination folder. If the source markdown has multiple mermaid visualizations then create multiple images. Name the images based on the source markdown file name appending -01, -02, -03 suffix sequentially if multiple visualizations found in same source file. Before creating the script research best image formats and conversion libraries for this task which yield highest quality images efficiently.
    **Completed**: Researched and implemented a superior local Mermaid rendering solution using the official Mermaid CLI via mermaid-mcp for professional-grade image generation. Created scripts/mermaid_to_image.py with full functionality for converting Mermaid diagrams to PNG, SVG, and PDF formats using the mature, battle-tested mmdc (Mermaid CLI) tool. The script features intelligent diagram type inference, automatic handling of all Mermaid diagram types (flowcharts, timelines, sequence diagrams, class diagrams, etc.), and configurable output settings. Created config.yml with user-configurable defaults for dimensions (1920x1080), themes (default, dark, forest, neutral, base), background colors, and timeout settings. Implemented proper directory structure matching source file locations (visualizations/parent-folder-name/), sequential filename generation (basename-01.png, basename-02.svg, etc.), comprehensive dependency checking, and robust error handling. Added mermaid-mcp dependency to pyproject.toml and tested successfully with multiple diagram types and formats. This approach is superior to browser automation as it uses the official Mermaid toolchain, providing more reliable rendering, better compatibility, and professional quality output suitable for documentation and presentations. Requires npm install -g @mermaid-js/mermaid-cli as system dependency.

[x] Fix HTML downloader to properly capture Substack article images wrapped in anchor tags
    **Completed**: Fixed the HTML downloader script's image extraction functionality to properly capture Substack-specific image patterns. Substack wraps images in anchor tags with class="image-link" where the actual image URLs are in the href attributes. Updated the find_images_in_content() function in scripts/html_downloader.py to additionally search for anchor tags with class="image-link" and extract their href attributes as image URLs. The fix now successfully downloads all images from Substack articles (31 images downloaded from the test article compared to 0 previously). The enhanced function checks for various image file extensions (.png, .jpg, .jpeg, .gif, .webp) and the substackcdn.com domain to ensure only actual images are captured from these anchor tags.

[x] Fix HTML downloader to convert Substack anchor-wrapped images to proper img tags for markdown conversion
    **Completed**: Enhanced the update_image_references() function in scripts/html_downloader.py to properly convert Substack's anchor-wrapped images (class="image-link") into actual <img> tags with local image references. The function now not only updates existing <img> tags but also finds anchor tags with class="image-link" that reference downloaded images and replaces them with proper <img> tags pointing to the local images. This ensures that when article_to_md.py processes the HTML, it finds proper <img> tags and can successfully extract and copy all images. Tested with a Substack article containing 31 images - all images are now properly converted to markdown with local image references preserved.

[x] Install timeout, ruff, and other tools you use frequently to test Python scripts. Add these to CLAUDE.md
    **Completed**: Installed and configured comprehensive Python development toolset as dev dependencies in pyproject.toml. Added ruff (v0.8.4+) for fast linting with 120-char line length configuration, black (v24.11.0+) for consistent code formatting, mypy (v1.14.1+) for static type checking, pytest (v8.3.5+) with pytest-timeout (v2.3.1+) for testing with 300s default timeout, and ipdb (v0.13.13+) for interactive debugging. Updated CLAUDE.md with comprehensive documentation including "Code Quality Tools" section with usage examples for all tools, "Development Tools" section listing all installed tools with versions and descriptions, and enhanced "Development Setup" section with dev dependency installation commands. All tools tested successfully - ruff detects import issues, black identifies formatting needs, mypy performs type checking, and pytest runs with timeout support.

[x] Create a scripts/10k_tech.py which takes a company name as input and downloads the latest 10k SEC filing in a temp folder. Research the best open API to use to download the 10k filing most efficiently and reliably. Then the script extracts only the minimal number of sections required to understand technology strategy and needs of the business. Use your understanding of 10k filings to apply a recipe to identify those sections in a generalized way for any company 10k. Make this identification recipe configurable using `config.yml` at project root. The script converts these sections into well-formatted valid markdown and saves in markdown/company/10k-yyyy.md file.
    **Completed**: Created comprehensive scripts/10k_tech.py script for extracting technology-relevant sections from SEC 10-K filings. Researched and implemented SEC's official API (data.sec.gov) with proper rate limiting (8 req/sec) and SEC-compliant user agent format. Script supports company name or ticker lookup via company_tickers.json, automatically finds latest 10-K filing, downloads HTML content, and extracts 4 key sections: Item 1 (Business), Item 1A (Risk Factors), Item 2 (Properties), and Item 7 (Management Discussion & Analysis). Extended config.yml with comprehensive sec_10k configuration including API settings, section patterns with regex matching, content filtering rules, and technology keywords. Implemented robust content extraction with proper section boundary detection, content cleaning, and markdown conversion with YAML frontmatter metadata. Output structure follows markdown/company-name/10k-yyyy.md pattern. Successfully tested with Microsoft (extracted 1,519 words across 4 sections from 2025 filing). Script includes proper error handling, progress feedback, type hints, and follows project coding standards.